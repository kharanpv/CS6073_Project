{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFX83fQ_vznh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    '''\n",
        "    Implementation of Self-attention. Usable by generator as well as discriminator\n",
        "    '''\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.Q = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.K = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.V = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        \n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Compute attenttion score as softmax(QK^T/sqrt(dim_k))*V\n",
        "        '''\n",
        "        batch_dim, C, H, W  = input.size()\n",
        "        \n",
        "        # Make input image to 1-D\n",
        "        query   = self.Q(input).view(batch_dim, -1, H * W).permute(0, 2, 1)\n",
        "        key     = self.K(input).view(batch_dim, -1, H * W)\n",
        "        value   = self.V(input).view(batch_dim, -1, H * W)  \n",
        "         \n",
        "        energy      = torch.bmm(query, key)\n",
        "        attention   = F.softmax(energy, dim=-1)\n",
        "\n",
        "        # [Batch, C, N_pixels] x [Batch, N_pixels, N_pixels] -> [Batch, C, H, W]\n",
        "        output  = torch.bmm(value, attention.permute(0, 2, 1)).view(batch_dim, C, H, W)\n",
        "        output  = self.gamma * output + input\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHKBmh9zyILJ"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator of the Pix2Pix model.\n",
        "       For the Lab version, nb_output_channels=2\n",
        "       For the RGB version, nb_output_channels=3\"\"\"\n",
        "    def __init__(self, nb_output_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leakyrelu = nn.LeakyReLU()\n",
        "        if nb_output_channels == 2:\n",
        "          self.activation = nn.Tanh()\n",
        "        elif nb_output_channels == 3:\n",
        "          self.activation = nn.Sigmoid()\n",
        "        \n",
        "        # ENCODER\n",
        "          \n",
        "        self.conv2d_1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_1 = nn.BatchNorm2d(64)\n",
        "        self.conv2d_2 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_2 = nn.BatchNorm2d(128)\n",
        "        self.conv2d_3 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_3 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        # Attention Block\n",
        "        self.attn   = SelfAttention(256)\n",
        "        \n",
        "        self.conv2d_4 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_4 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_5 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_5 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_6 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_6 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_7 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_7 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_8 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "\n",
        "        # DECODER\n",
        "        \n",
        "        self.conv2d_9 = nn.ConvTranspose2d(in_channels=512,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_9 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_10 = nn.ConvTranspose2d(in_channels=512*2,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_10 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_11 = nn.ConvTranspose2d(in_channels=512*2,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_11 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_12 = nn.ConvTranspose2d(in_channels=512*2,out_channels=512,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_12 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_13 = nn.ConvTranspose2d(in_channels=512*2,out_channels=256,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_13 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv2d_14 = nn.ConvTranspose2d(in_channels=256*2,out_channels=128,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_14 = nn.BatchNorm2d(128)\n",
        "        self.conv2d_15 = nn.ConvTranspose2d(in_channels=128*2,out_channels=64,kernel_size=4,stride=2, padding=1, bias=False)\n",
        "        self.batchnorm_15 = nn.BatchNorm2d(64)\n",
        "        self.conv2d_16 = nn.ConvTranspose2d(in_channels=64*2,out_channels=nb_output_channels,kernel_size=4,stride=2, padding=1, bias=True)\n",
        "\n",
        "    def forward(self, encoder_input):\n",
        "        # encoder\n",
        "        encoder_output_1 = self.leakyrelu(self.conv2d_1(encoder_input))\n",
        "        encoder_output_2 = self.leakyrelu(self.batchnorm_2(self.conv2d_2(encoder_output_1)))\n",
        "        encoder_output_3 = self.leakyrelu(self.batchnorm_3(self.conv2d_3(encoder_output_2)))\n",
        "        \n",
        "        # attention\n",
        "        encoder_output_3 = self.attn(encoder_output_3)\n",
        "        \n",
        "        encoder_output_4 = self.leakyrelu(self.batchnorm_4(self.conv2d_4(encoder_output_3)))\n",
        "        encoder_output_5 = self.leakyrelu(self.batchnorm_5(self.conv2d_5(encoder_output_4)))\n",
        "        encoder_output_6 = self.leakyrelu(self.batchnorm_6(self.conv2d_6(encoder_output_5)))\n",
        "        encoder_output_7 = self.leakyrelu(self.batchnorm_7(self.conv2d_7(encoder_output_6)))\n",
        "        encoder_output   = self.conv2d_8(encoder_output_7)\n",
        "        \n",
        "        # decoder\n",
        "        decoder_output = self.batchnorm_9(self.conv2d_9(self.relu(encoder_output)))\n",
        "        decoder_output = self.batchnorm_10(self.conv2d_10(self.relu(torch.cat([encoder_output_7,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.batchnorm_11(self.conv2d_11(self.relu(torch.cat([encoder_output_6,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.batchnorm_12(self.conv2d_12(self.relu(torch.cat([encoder_output_5,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.batchnorm_13(self.conv2d_13(self.relu(torch.cat([encoder_output_4,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.batchnorm_14(self.conv2d_14(self.relu(torch.cat([encoder_output_3,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.batchnorm_15(self.conv2d_15(self.relu(torch.cat([encoder_output_2,decoder_output],1)))) #skip connection\n",
        "        decoder_output = self.activation(self.conv2d_16(self.relu(torch.cat([encoder_output_1,decoder_output],1)))) #skip connection\n",
        "        return decoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFM5imQyyRil"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Patch discriminator of the Pix2Pix model.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        # Encoder\n",
        "        self.conv2d_1 = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=4,stride=2,padding=1, bias=True)\n",
        "        self.conv2d_2 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=4,stride=2,padding=1, bias=False)\n",
        "        self.batchnorm_2 = nn.BatchNorm2d(128)\n",
        "        self.conv2d_3 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4,stride=2,padding=1, bias=False)\n",
        "        self.batchnorm_3 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        # Attention\n",
        "        self.attn   = SelfAttention(256)\n",
        "         \n",
        "        # Decoder\n",
        "        self.conv2d_4 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=4,stride=1,padding=1, bias=False)\n",
        "        self.batchnorm_4 = nn.BatchNorm2d(512)\n",
        "        self.conv2d_5 = nn.Conv2d(in_channels=512,out_channels=1,kernel_size=4,stride=1,padding=1,bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.leakyrelu(self.conv2d_1(input))\n",
        "        output = self.leakyrelu(self.batchnorm_2(self.conv2d_2(output)))\n",
        "        output = self.leakyrelu(self.batchnorm_3(self.conv2d_3(output)))\n",
        "        \n",
        "        # Attention\n",
        "        output = self.attn(output)\n",
        "        \n",
        "        output = self.leakyrelu(self.batchnorm_4(self.conv2d_4(output)))\n",
        "        output = self.sigmoid(self.conv2d_5(output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OpQb58BGyeqX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def init_weights(m, gain=0.02):\n",
        "  \"\"\"weight initialisation of the different layers of the Generator and Discriminator\"\"\"\n",
        "  if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
        "    nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "    if m.bias is not None:\n",
        "        nn.init.constant_(m.bias.data, 0.0)\n",
        "  elif type(m) == nn.BatchNorm2d:\n",
        "    nn.init.normal_(m.weight.data, 1., gain)\n",
        "    nn.init.constant_(m.bias.data, 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "na_nC6IxyfIL"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorLoss(nn.Module):\n",
        "  \"\"\"for the patch discriminator, the output is a 30x30 tensor\n",
        "     if the image is real, it should return all ones 'real_labels'\n",
        "     if the image is fake, it should return all zeros 'fake_labels' \n",
        "     returns the MSE loss between the output of the discriminator and the label\"\"\"\n",
        "  def __init__(self, device):\n",
        "      super().__init__()\n",
        "      self.register_buffer('real_labels', torch.ones([30,30], requires_grad=False, device=device), False)\n",
        "      self.register_buffer('fake_labels', torch.zeros([30,30], requires_grad=False, device=device), False)\n",
        "      #use MSE loss for the discriminator\n",
        "      self.loss = nn.MSELoss()\n",
        "\n",
        "  def forward(self, predictions, target_is_real):\n",
        "        if target_is_real:\n",
        "            target = self.real_labels\n",
        "        else:\n",
        "            target = self.fake_labels\n",
        "        return self.loss(predictions, target.expand_as(predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pix2pix_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
